{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with LEIP Recipes\n",
    "An End to End tutorial\n",
    "---\n",
    "\n",
    "In this notebook we will run through an example customer use case end to end.\n",
    "Starting from just having an annotated dataset, we will get some recipes from the Golden Recipes database that are likely to perform well on our data, train them, and check the actual performance of our trained models in framework before exporting the best one. \n",
    "Then we will optimize and compile our models for our example hardware: a CUDA enabled NVIDIA device.\n",
    "\n",
    "Our example data today is Road Sign Detection from kaggle.\n",
    "\n",
    "The steps we will follow in this tutorial are:\n",
    "1. Download the Road Sign Detection dataset from kaggle, and get familiar with it.\n",
    "2. Select some candidate recipes: \n",
    "    - Explore the Golden Recipe Volumes available today.\n",
    "    - Select a volume of Golden Recipes that is most likely to be a good match for our dataset.\n",
    "    - Pick 10 candidate recipes to try on our Road Sign Detection data.\n",
    "3. Train the candidate recipes on the Road Sign Detection data.\n",
    "4. Evaluate the models trained using the candidate recipes, select the best performer, and export it.\n",
    "5. Optimize the best performer and compile it for deployment.\n",
    "6. Evaluate the optimized model to ensure enough accuracy has been preserved.\n",
    "\n",
    "## Set Up\n",
    "\n",
    "If you have not done so already, please ensure your environment is set up according to [Setting up a LEIP Environment](../environment/README.md). This will ensure that while executing this notebook the APIs of both the [Application Framework](https://leipdocs.latentai.io/af/latest/content/) and [Compiler Framework](https://leipdocs.latentai.io/cf/latest/content/) are accessible.\n",
    "\n",
    "## Initialization\n",
    "\n",
    "Here we initialize some variables and ensure `leip_client` is able to connect to the server side container. A green checkbox printed after `Checking connection` signifies a successful connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from leip_client import Leip\n",
    "from rich import print\n",
    "\n",
    "# Create a LEIP instance\n",
    "leip = Leip.load_instance()\n",
    "host = os.environ.get(\"LEIP_CF_HOST\", \"http://127.0.0.1:8080\")\n",
    "\n",
    "# Configure connection to CF container\n",
    "leip.create_profile(\"default\", host, os.environ[\"LICENSE_KEY\"])\n",
    "leip.check_connection(silent=False)\n",
    "\n",
    "# Initialize shared workspace volume\n",
    "assert os.environ.get(\"LEIP_WORKSPACE\"), \"Make sure your environment variable LEIP_WORKSPACE is set according to the environment setup instructions.\"\n",
    "workspace = Path(os.environ.get(\"LEIP_WORKSPACE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get the dataset\n",
    "\n",
    "Download the Road Sign Detection dataset from kaggle, and get familiar with it: open [the dataset page on Kaggle](https://www.kaggle.com/datasets/andrewmvd/road-sign-detection) and click \"download (229mb)\". This may require signing in to Kaggle using your own credentials. Then unzip the downloaded file to `$LEIP_WORKSPACE/datasets/kaggle/road-sign-data`.\n",
    "\n",
    "For your convenience, we have mirrored the dataset so that you can download and unzip by running this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import shutil\n",
    "\n",
    "dataset_dir = workspace / \"datasets\" / \"kaggle\" / \"road-sign-data\"\n",
    "\n",
    "if not dataset_dir.exists():\n",
    "    mirror_url = \"https://s3.us-west-1.amazonaws.com/leip-showcase.latentai.io/recipes/andrewmvd_road-sign-detection.zip\"\n",
    "    local_file = Path(\"andrewmvd_road-sign-detection.zip\")\n",
    "\n",
    "    print(\"Downloading dataset...\")\n",
    "\n",
    "    response = requests.get(mirror_url)\n",
    "    with open(local_file, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    shutil.unpack_archive(local_file, dataset_dir)\n",
    "\n",
    "    print(f\"Dataset now available at {dataset_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To design recipes, we need ingredients. Ingredients live in the pantry. A pantry can have all sorts of ingredients, from models to datasets, optimizers and learning rate schedulers, and even training-aware quantization techniques. Let's download a pantry full of useful ingredients to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from leip_recipe_designer import Pantry\n",
    "pantry = Pantry.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ingest our Road Sign Data, we need a data_generator ingredient for it. A data_generator ingredient tells the execution where from and how to ingest the dataset.\n",
    "\n",
    "The `new_pascal_data_generator` helper function will help us create the data_generator ingredient for our Road Sign Data, so we can use it in our recipes. \n",
    "\n",
    "_Tip: This function won't execute any task; instead, its whole purpose is to help us generate the ingestion ingredient correctly. Once generated, the ingredient will be added to a recipe, and the `visualize_data` task will be run on that recipe._\n",
    "\n",
    "Make some observations about the folder structure and format of the data, so we can properly create the data_generator ingredient:\n",
    "- The data is in the **Pascal VOC format**, so we will use the `new_pascal_data_generator` helper function.\n",
    "- The annotations are in XML files under a folder called `annotations` and images under an `images` folder. We will enter that information for `annotations_dir` and `images_dir`.\n",
    "- The dataset appears to **not be pre-split** into a training and validation set. We will set `is_split = False`.\n",
    "- There are **4 object classes** in our data. We will set `nclasses = 4`.\n",
    "- The dataset contains a *total of 877 samples*. Images have low resolutions with varying aspect ratios; some landscape and some portrait oriented. This information does not go into the data_generator, but it may help us find the most appropriate golden recipes in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from leip_recipe_designer.helpers.data import new_pascal_data_generator\n",
    "\n",
    "road_sign_data = new_pascal_data_generator(\n",
    "    pantry=pantry,\n",
    "    root_path=str(dataset_dir),\n",
    "    images_dir=\"images\",\n",
    "    annotations_dir=\"annotations\",\n",
    "    nclasses=4,\n",
    "    is_split=False,\n",
    "    trainval_split_ratio=0.80,\n",
    "    dataset_name=\"road-sign-data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks such as `visualize_data` run on recipes. All we have at this point is one ingredient. Let's start a blank recipe, fill it with only the essentials, and add this data_generator ingredient to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from leip_recipe_designer.create import empty_detection_recipe\n",
    "my_temporary_recipe = empty_detection_recipe(pantry=pantry)\n",
    "my_temporary_recipe.fill_empty_recursively()\n",
    "\n",
    "from leip_recipe_designer.helpers.data import replace_data_generator\n",
    "replace_data_generator(recipe=my_temporary_recipe, data=road_sign_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from leip_recipe_designer import tasks\n",
    "vizdata_outputs = tasks.visualize_data(my_temporary_recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All tasks return a dictionary with any useful outputs of the task. In the case of the `visualize_data` task, the dictionary contains the directories where the sample images were stored. Let's print it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(vizdata_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write a little function to display the saved images in this Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "def display_helper(images_path, count=4, image_extension=\"jpeg\"):\n",
    "    fig = plt.figure(figsize=(20, 10), facecolor='w')\n",
    "    columns = 4\n",
    "    rows = math.ceil(count / columns)\n",
    "    for idx, path in zip(range(rows * columns), Path(images_path).rglob(str(\"*.\"+image_extension))):\n",
    "        img = mpimg.imread(path)\n",
    "        fig.add_subplot(rows, columns, idx + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "print(\"Some sample training images:\")\n",
    "display_helper(vizdata_outputs[\"vizdata.output_directory.train\"], count=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"Some sample validation images:\")\n",
    "display_helper(vizdata_outputs[\"vizdata.output_directory.val\"], count=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Select candidate recipes\n",
    "\n",
    "Golden Recipe volumes are collections of recipes that have proven to perform well on a particular dataset. Latent AI has computers churning away discovering better recipes for more and more diverse datasets.\n",
    "\n",
    "Let's list the available volumes as of today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from leip_recipe_designer import GoldenVolumes\n",
    "goldenvolumes = GoldenVolumes()\n",
    "volumes = goldenvolumes.list_volumes_from_zoo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the above names correspond to a Golden Recipe Volume: a set of recipes that performed well on a particular dataset. We can explore the datasets used to train each of the volumes and try to find the one that comes closest to our data. \n",
    "\n",
    "In our case, our data is simple, with less than 1000 samples, and contains outdoor images of sparse objects with resolutions around the 300x400 and 400x300 range.\n",
    "\n",
    "If any of the volumes strike you as having data that is really similar to yours, go ahead and start with that. Note that data similarity is not a clearly defined concept, so pick one volume to start, and iterate from there. For more information read [How to Pick the Best GRDB Volume to Start](/af/latest/content/getting-started/how_to/#pick-the-best-grdb-volume-to-start).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the GRDBs is a car detection data. It is a very small dataset of objects with simple shapes. Perhaps that is a good place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "volumes[\"carsimple\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good performance can mean different things to different people: task accuracy is a good measure of performance, but maybe you care more about fast training than getting the most accuracy possible, or perhaps having a small model that runs inference very quickly in your device of interest is imperative for your application.\n",
    "\n",
    "Because of this, Golden Recipe volumes are structured as Pandas dataframes that you can query and sort according to your requirements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df = volumes[\"carsimple\"].get_golden_df()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, we will keep things simple: we want candidate recipes that do not sacrifice too much accuracy and are also likely to run fast once compiled (based on the number of Multiply-Accumulate Operations during inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "threshold = df[\"if_task_metric\"].quantile(0.75)\n",
    "filtered_df = df[df[\"if_task_metric\"] >= threshold]\n",
    "sorted_df = filtered_df.sort_values(\"if_inf_macs\")\n",
    "sorted_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve the SPPR (Serialized Packaged Portable Recipe) from each of the results. To deserialize, use `from_sppr`.\n",
    "We don't have unlimited amounts of time to train, so we will limit our list of candidate recipes for training to the best 4. \n",
    "\n",
    "Once you deserialize, training the candidate recipes only requires swapping the data ingredient of each of the candidate recipes with your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from leip_recipe_designer.create import from_sppr\n",
    "\n",
    "optimal_models = [\"GRDBCAR-9\", \"GRDBCAR-213\", \"GRDBCAR-66\", \"GRDBCAR-12\", \"GRDBCAR-166\"]\n",
    "\n",
    "# Extract the recipes\n",
    "candidate_recipes = {}\n",
    "for recipe_id in optimal_models:\n",
    "    sppr = sorted_df.query(f\"id == '{recipe_id}'\").iloc[0][\"sppr\"]\n",
    "    candidate_recipes[recipe_id] = from_sppr(sppr, pantry, allow_upgrade=True)\n",
    "print(f\"We have collected the {len(candidate_recipes)} recipes with lowest MACs that meet our task metric performance criteria\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 3: Train the candidate recipes\n",
    "\n",
    "_Tip: no need to wait for training!_\n",
    "\n",
    "Training these recipes until convergence will take several minutes: an NVIDIA RTX A4500 GPU took about 40 minutes to train these 4 models. We recognize you may want to continue with the tutorial without having to wait until convergence, so we've added a few lines below to limit training time to only one tenth of an epoch. After this short training showcase, a few lines of code below will download the trained checkpoints, so you can move to the next step of evaluating the recipes.\n",
    "\n",
    "**If you prefer to wait until training converges instead of downloading the checkpoints, remove `recipe[\"train.num_epochs\"] = 1; recipe[\"trainer.train_batches_percentage\"] = 0.1`. These lines limit training to just 10% of one epoch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from leip_recipe_designer.helpers.data import replace_data_generator\n",
    "from leip_recipe_designer import tasks\n",
    "\n",
    "train_outputs = {}\n",
    "for recipe_id, recipe in candidate_recipes.items():\n",
    "    # it is necessary to add a logger to our run. Below we are adding a default local log.\n",
    "    # If you use a different logger such as Weights and Biases or Neptune,\n",
    "    # visit our documentation for instructions on how to add it to the recipe\n",
    "    recipe.assign_ingredients('loggers', {\"my_local_training_log\": \"Tensorboard\"})\n",
    "\n",
    "    # in place change in the recipe. Swaps the recipe's original training data with your data\n",
    "    replace_data_generator(recipe, road_sign_data)\n",
    "    try:\n",
    "        # change the mosaic augmentation sampling scheme to tolerate different input resolution\n",
    "        recipe[\"data_generator.composite.mosaic.sampling_scheme\"] = \"preserve_spatial\"\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    # THE BELOW LINE CUTS TRAINING SHORT, FOR THE SAKE OF TIME. IF YOU HAVE TIME TO WAIT UNTIL CONVERGENCE,\n",
    "    # COMMENT THE LINE BELOW AND THE MODEL WILL STOP AUTOMATICALLY ONCE IT'S DONE TRAINING\n",
    "    recipe[\"train.num_epochs\"] = 1\n",
    "    recipe[\"trainer.train_batches_percentage\"] = 0.1\n",
    "\n",
    "    # This is a completely optional step. Since we are training multiple recipes, I will use the recipe ID to identify the artifacts generated by this recipe\n",
    "    recipe[\"experiment.name\"] = recipe_id\n",
    "\n",
    "    print(f\"\\n\\nTraining recipe {recipe_id}\")\n",
    "    train_output = tasks.train(recipe)\n",
    "    train_outputs[recipe_id] = train_output\n",
    "\n",
    "    # After training is finished for a recipe, add the checkpoint path to the recipe, so it can be used by the evaluate and export tasks below\n",
    "    recipe[\"model.checkpoint\"] = str(train_output[\"best_model_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Download the checkpoints\n",
    "\n",
    "After waiting for the short training to complete above, you will have training outputs. As mentioned earlier in this tutorial, all tasks return a dictionary pointing to any outputs generated from the task.\n",
    "\n",
    "Unless you commented out the specified lines, you only trained for 10% of an epoch, so we don't expect your model to have learned much. The cells below will download and extract the checkpoints that were trained in Latent AI servers, and modify your training outputs to point to the downloaded checkpoints instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace these values with the URL of the file you want to download\n",
    "file_url = \"https://s3.us-west-1.amazonaws.com/leip-showcase.latentai.io/recipes/tutorials/GettingStartedCheckpoints.zip\"\n",
    "\n",
    "# Specify the local directory to save the downloaded and extracted files\n",
    "local_directory = Path(\"downloaded_checkpoints\")\n",
    "\n",
    "# Create the local directory if it doesn't exist\n",
    "local_directory.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Path to save the downloaded file\n",
    "zip_file_path = local_directory / \"GettingStartedCheckpoints.zip\"\n",
    "\n",
    "# Download the file\n",
    "response = requests.get(file_url, stream=True)\n",
    "if response.status_code == 200:\n",
    "    with open(zip_file_path, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "# Extract the contents of the zip file\n",
    "shutil.unpack_archive(zip_file_path, local_directory)\n",
    "\n",
    "print(\"Downloaded and extracted files to:\", local_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The cell below replaces the trained checkpoints with the downloaded ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "file_paths = []\n",
    "for root, dirs, files in os.walk(local_directory):\n",
    "    for filename in files:\n",
    "        file_path = os.path.abspath(os.path.join(root, filename))\n",
    "        file_paths.append(file_path)\n",
    "\n",
    "for recipe_id, recipe in candidate_recipes.items():\n",
    "    for file_path in file_paths:\n",
    "        if recipe_id in file_path and \"ckpt\" in file_path:\n",
    "            recipe[\"model.checkpoint\"] = file_path\n",
    "            print(\"Added the downloaded checkpoint from\", file_path, \"to the correct recipe\", recipe_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Pick a winner\n",
    "\n",
    "Evaluate the models trained using the candidate recipes, and select the best performer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "evaluate_outputs = {}\n",
    "for recipe_id, recipe in candidate_recipes.items():\n",
    "    eval_output = tasks.evaluate(recipe)\n",
    "    evaluate_outputs[recipe_id] = eval_output\n",
    "\n",
    "for recipe_id, scores in evaluate_outputs.items():\n",
    "    print(\"Recipe with ID\", recipe_id, \"has a Mean Average Precision score (averaged over IoU Thresholds 0.50:.95:0.05) of\", scores[\"evaluate.metric_single\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The highest task accuracy was obtained by the recipe with ID GRDBCAR-213**\n",
    "\n",
    "We can visualize its predictions on the data using the helper we defined earlier in this tutorial. This is a completely optional step to ensure things look good before we export the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_recipe = candidate_recipes[\"GRDBCAR-213\"]\n",
    "predict_output = tasks.visualize_predictions(best_recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "display_helper(predict_output[\"predict.output_directory\"], count=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to export our recipe as a traced artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model_path = workspace / \"exported\"\n",
    "best_recipe[\"export.output_directory\"] = str(model_path)\n",
    "export_output = tasks.export_model(best_recipe)\n",
    "print(f\"Your model has been saved to disk under {export_output['export.model_path']}.\\nYou can now move forward with optimizing your model using the LEIP Compiler Framework.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Optimize Your Recipe\n",
    "\n",
    "Next we will optimize the model exported in the last section. We will use the Pipeline API to group together a couple of subtasks into one batched execution. One subtask will optimize and compile the model into `int8` for its target, and the other subtask will compile to `float32`.\n",
    "\n",
    "We will set a few variables that will be used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = workspace / \"datasets\" / \"kaggle\" / \"road-sign-data\"\n",
    "target = \"cuda\"\n",
    "target_host = \"llvm -mcpu=skylake\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will run LEIP Pipeline. The `model_options`, `compress_options` and `compile_options` together comprise the various options needed to optimize and compile the model. Refer to the [Compiler Framework documentation](https://leipdocs.latentai.io/cf/latest/content/) for more information on the available options.\n",
    "\n",
    "The optimize step requires a *representative dataset* which is used for calibration during the quantization of activations in the model. A good rule of thumb for creating a representative dataset is to include at least one image from each output class in the dataset. Often though, even a few input samples from the dataset are enough to get good quantized accuracy for the model. Other times the success of the quantization can be very sensitive to the representative dataset chosen. So you might want to experiment in this area. We will choose a simple representative dataset of 10 randomly selected images for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_dataset_paths = [\n",
    "    dataset_dir / \"images\" / \"road114.png\",\n",
    "    dataset_dir / \"images\" / \"road14.png\",\n",
    "    dataset_dir / \"images\" / \"road214.png\",\n",
    "    dataset_dir / \"images\" / \"road314.png\",\n",
    "    dataset_dir / \"images\" / \"road414.png\",\n",
    "    dataset_dir / \"images\" / \"road514.png\",\n",
    "    dataset_dir / \"images\" / \"road614.png\",\n",
    "    dataset_dir / \"images\" / \"road714.png\",\n",
    "    dataset_dir / \"images\" / \"road814.png\",\n",
    "    dataset_dir / \"images\" / \"road77.png\"\n",
    "]\n",
    "rep_dataset_items = [\n",
    "    str(path) for path in rep_dataset_paths\n",
    "]\n",
    "rep_dataset_file = workspace / \"rep_dataset.txt\"\n",
    "rep_dataset_file.write_text(\"\\n\".join(rep_dataset_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell runs the pipeline. Please note that the pipeline might take a few minutes to run the optimization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from leip_client import (\n",
    "    TaskModelOptions,\n",
    "    CompressOptions,\n",
    "    CompileOptions,\n",
    "    PipelineTask,\n",
    "    PipelineInnerTask,\n",
    "    CompilePipelineOptions,\n",
    "    OptimizePipelineOptions,\n",
    "    OutputOptions,\n",
    ")\n",
    "\n",
    "model_options = TaskModelOptions(\n",
    "    path=model_path,\n",
    "    task_family=\"detection\",\n",
    ")\n",
    "\n",
    "compress_options = CompressOptions(\n",
    "    rep_dataset=rep_dataset_file,\n",
    "    quantizer=\"symmetricpc\",\n",
    ")\n",
    "\n",
    "compile_options = CompileOptions(\n",
    "    target=target,\n",
    "    target_host=target_host,\n",
    ")\n",
    "\n",
    "pipeline_config = PipelineTask(\n",
    "    name=\"RecipePipeline\",\n",
    "    tasks=[\n",
    "        PipelineInnerTask(\n",
    "            name=\"Int8\",\n",
    "            optimize=OptimizePipelineOptions(\n",
    "                model=model_options,\n",
    "                compress=compress_options,\n",
    "                compile=compile_options,\n",
    "            ),\n",
    "        ),\n",
    "        PipelineInnerTask(\n",
    "            name=\"Float32\",\n",
    "            compile=CompilePipelineOptions(\n",
    "                model=model_options,\n",
    "                target=compile_options.target,\n",
    "                target_host=compile_options.target_host,\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    "    output=OutputOptions(path=workspace)\n",
    ")\n",
    "\n",
    "print(\"Optimizing model...\")\n",
    "pipeline_job = pipeline_config.run()\n",
    "print(pipeline_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate\n",
    "\n",
    "Now we will use the Evaluate API to run an evaluation of the optimized model on the server. This will return a mean average precision (mAP) for how well the optimized model does on the validation set of the dataset in our detection task. Note that the mAP score might not be as good as the scores above when evaluated in-framework, but it should not have dropped significantly.\n",
    "\n",
    "Refer to [the LEIP Evaluate module](https://leipdocs.latentai.io/cf/latest/content/modules/evaluate/) for more information on using LEIP Evaluate for evaluating the accuracy of a model throughout the various stages in your pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from leip_client import (\n",
    "    DatasetOptions,\n",
    "    PascalVOCOptions,\n",
    "    EvaluateTask,\n",
    ")\n",
    "from leip_zoo import DatasetSchema\n",
    "\n",
    "int8_model = workspace / \"Int8-optimize\"\n",
    "\n",
    "best_recipe.assign_ingredients(\"export_data\", \"COCO\")\n",
    "best_recipe[\"export_data.subset\"] = \"val\"\n",
    "best_recipe[\"export_data.save_directory\"] = str(workspace / \"datasets\" / \"kaggle\")\n",
    "export_data_output = tasks.export_data(best_recipe)\n",
    "schema_file = Path(export_data_output[\"export_data.output_directory\"]) / \"dataset_schema_val.yaml\"\n",
    "dataset_options = DatasetSchema.load(schema_file, options={\"root_dir\": schema_file.parent}).options\n",
    "\n",
    "evaluate_task = EvaluateTask(\n",
    "    model=TaskModelOptions(path=int8_model),\n",
    "    dataset=dataset_options,\n",
    ")\n",
    "\n",
    "print(\"Evaluating optimized int8 model...\")\n",
    "model_eval_job = evaluate_task.run()\n",
    "print(model_eval_job)\n",
    "mAP = model_eval_job.results[\"scoring\"][\"mAP\"][\"0.5:0.95:0.05\"]\n",
    "print(f\"int8 mAP: {mAP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that we have optimized and compiled our model (using LEIP Optimize) and evaluated how well the model performs (using LEIP Evaluate) from a host environment, we are ready to deploy the model on an edge device (`cuda` hosted by an `llvm -mcpu=skylake` for this tutorial).\n",
    "\n",
    "To streamline the process of transferring the optimized model for deployment, we'll assemble all model-related files into a single tar file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open(workspace / \"int8_model.tar.gz\", \"w:gz\") as tar:\n",
    "    for file in int8_model.glob(\"*\"):\n",
    "        tar.add(file, arcname=file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also copy an image to test on target device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp $LEIP_WORKSPACE/datasets/kaggle/road-sign-data/images/road314.png $LEIP_WORKSPACE/road314.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create a labels file for visualization on the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values=$(jq -r 'values | .[]' < \"$LEIP_WORKSPACE/datasets/kaggle/road-sign-data/pascal_label_map.json\")\n",
    "echo \"background\" > \"$LEIP_WORKSPACE/roadsign.txt\" # leave an empty line for index 0 which maps to background, overwrite an existing file\n",
    "echo \"$values\" | tr '\\n' '\\n' >> \"$LEIP_WORKSPACE/roadsign.txt\" # add each label as a new line after the empty line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the artifacts from our host enviroment that we would like to take to our deployment environment.\n",
    "1. Compiled model and metadata: $LEIP_WORKSPACE/int8_model.tar.gz\n",
    "2. Test images: $LEIP_WORKSPACE/road314.png  \n",
    "3. Application artifacts: $LEIP_WORKSPACE/roadsign.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our deployment environment ready and artifacts transfered to the deployment device, we can deploy our compiled model with Python or C++. Since the models we selected are detection models we will use the detectors example. [Detectors example implementations](https://github.com/latentai/example-applications/tree/v3.0.1/detectors) provide details on how to deploy along with its implementation details and options to customize.\n",
    "\n",
    "We can use options provided in the detectors example to deploy the compiled model and visualize on the image we selected. Parameters used in the evaluation environment are available in `$INT8_MODEL/model_schema.yaml`. However, we can modify these parameters to adjust the visualization to display few useful detections on our selected image. Selection of these parameters have accuracy and latency implications. (For example, we ran for 10 iterations for steady state latency numbers, we used `maximum_detections` = 5, `confidence_threshold` = 0.6, `iou_threshold` = 0.45 to get most likely detections fast.)\n",
    "\n",
    "Deployment example applications are built using the [Runtime Framework](https://leipdocs.latentai.io/rf/content/) where you can find details about the API and its usage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
